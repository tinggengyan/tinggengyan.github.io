
# BTM
- [参考链接 1](https://www.biaodianfu.com/usertrack.html?hmsr=joyk.com&utm_source=joyk.com&utm_medium=referral)
## 背景
1. 【case】电商需要对搜索对于成单的导流进行归因统计；
2. 【case】在播放页，希望获取到搜索的关键词，这时搜索会去修改播放详情页的代码，易错；根据页面是否来自于搜索，添加搜索固有的参数，*来源* 的各方的含义不一致；
3. 【人力】每周会有一个埋点的需求，人力投入很大；
4. 【现象】DA  认为数据质量差，因为发版都可能导致数据异常；
5. 【现象】埋点数据乱象，不确定埋点含义；
6. 【case】搜索结果列表中的某个 card 需要获取整个页面的字段，需要深度传递；


## 冲突
1. 现有埋点体系、规范不统一，但是又难以修改，如同样是  play  埋点，为了区分是搜索导流的流量，单独上报了 search_play  埋点；
2. 各个业务的  DA  分析都有各自的口径，和埋点结构；
3. 有效利用流量，分析流量，以支撑运营、辅助决策；
4. 流量归属不明、采集困难、缺乏统一口径；
5. 多级链路定位困难，如搜索为电商的导流效果；


## 问题
1. 如何制定统一的流量规范，又能兼顾当前的埋点规范？
2. 如何让数据的口径统一？
3. 如何支撑数据的流量分析、归因分析？
4. 如何实现数据的广度和深度的传递？


## 解决方案

### 整体方案
1. 埋点评审和业务评审同步进行；
2. 元信息细化、补充、保障元信息的完整度；
3. 统一埋点数据规范；
4. 提供  SDK 保障埋点的数据口径统一；


###  统一埋点规范
- 全面、唯一、稳定的资源标记体系，对每个页面、每个可交互的控件进行唯一标记；
- 对于每个交互的动作进行分类，如视图曝光、页面  PV、点击事件等等；
- 半自动采集所有页面、交互的数据，实现内容交互数据的串联；
- 构建基础表，打造基础的链路能力；
- 针对基础数据，建设基于  spm  的各个维度的转化数据，可视化展示，如来源去向图、成单链路图等等；

> 由土地和人口的管理制度引发思考，对  App  内的流量进行分割定义，App  标记、页面标记、区块标记、点位标记。除了 App 标记，其他标记是有较强的主观性的。标记的格式为：a.b.c.d。

- 实现 activity 的生命周期的监听，在 onCreate  时，进行页面记录的构造；
	- 自身记录；
	- 获取前置页面的记录；
	- 串联行程链条；
- hook  activity  的启动，在打开 activity 时，将当前  activity  的记录塞进  intent  中，实现记录的有效传递，且能解决跨进程的问题；
- fragment  页面挂载在  activity  上，activity  会记录每次展示的  fragment；
- 针对页面的  low memory  被杀重建，将  record  序列化在  bundle  内；
- 针对自定义的构建页面，需要业务方手动调用对应的方法，自住构建页面记录，挂载到链路上；


### 链路传参方案
- 针对传参，延伸能力，提供责任链式参数维护方案；
- 【上下文】、【可响应能力】的框架；
- 替  activity 、fragment 、view 自动构建  scope  对象，scope  之间就天然形成树状结构，可以实现  scope  的回溯，在回溯的链路上任意获取数据；对于兄弟阶级的数据，可以通过将节点挂载到更高一级的节点上；
-  使用的时候，通过构建  *source* 作为数据源，再将  source  绑定到对应的  scope  即可；

## 成果
### 公司
- 将方案推广至字节多条产品线；
- 方案获得头部业务的认可；
- TT  内多个业务正在投入使用；
### 个人
- 提高了跨团队、跨职能的沟通能力；
- 提高技术方案设计能力；
- 提升了目标管理、任务拆解的能力；

# 用户动线
## 背景
- 【case】需要排查某个页面白屏，内容是根据接口的数据动态生成的，在没有现场的情况下，无法知晓原因；
- 【case】一个埋点传参出现大量的  *undefined*，这个字段是从上游页面传递下来的，没有现场，无法知晓是从哪个页面，因为什么原因而导致的。

## 冲突
1. 【复杂】用户行为的埋点和错误日志在不同的平台上，没法打通，查询的链路长，问题定位久；
2. 【效率】海外数据需要海外专职同学操作，流程太长，效率很低；
3. 【体验】现有的监控，都是建立在技术的视角，缺乏用户视角，无法反应线上的用户体验状况；

## 问题
- 如何打通行为数据、错误数据和网络数据？
- 如何如何将查询标准化、自动化，提高效率？
- 如何将数据可视化，反应用户的体验视角？

## 解决方案
> 自定义一套规范，将行为日志、普通日志、错误日志、server  日志进行关联聚合，再通过合适的方式进行可视化的展示。
- 客户端，改造埋点层、网络层
	- 在埋点 SDK 中添加 trace_id；
	- 在网络 SDK 中，在请求的  header  中添加  trace_id；
	- trace_id （固定的位数，分成  trace_id，biz_flag，parent_id,span_id,sample_flag
	- 每个操作节点对应一个  trace_id，比如进入页面，点击按钮等等；
- fe  侧，通过  jsb  将行为转接到  native  上；
- server  TLB，在接收到网络请求后，解析  trace_id ,并生成此次的  log_id，将二者的映射关系进行存储；
- 前端 console，提供按照 uid、did、时间进行筛选的能力，展示以树状图的形式。首先以时间为维度进行排序，再辅以  trace_id  的方式进行聚合，叠层展示。

## 成果
- 与中台达成合作共识，一期落地，数据打通完成，前端展示完成，整体流程初步跑通了；

# 埋点治理
## 背景
降本增效为公司三年的方向，TT  从创建至今近万个埋点，没有进行过专项的治理。且  2023  春节期，存储机器就将全部枯竭。

## 冲突
- 单个埋点的成本不知如何计算；
- 埋点的源头是业务方，业务方才知晓；
- 埋点的责任人难以确认，元信息维护不足以支撑埋点可以确认埋点是否可以下线；
- 业务研发很忙，如何提升专项的重要性，提升大家的积极性和配合度；
- 埋点的重要性很高，但是链路非常长，消费方很多，溯源困难；

## 问题
1. 埋点的成本由哪些组成部分?该如何计算？
2. 埋点的责任人如何确认？
3. 如何提升大家的配合度？
4. 如何保障稳定性，确保数据安全；


## 解决方案
1. 梳理埋点整个体系流程，请求基础数仓的协助，对应的成本计算交由专业的基础数仓决定；
2. 请求数据消费平台协助，拉取所有埋点元数据，将其作为主要参考依据；
3. 将埋点下线拆分几个阶段：消费下线  -> db  下线 ->流量下线 -> 配置开关下线 -> 代码下线，稳步进行；
4. 将提案上升至技术委员会，争取技术委员会的支持和协助；


## 成果
1. 年节省  1  亿；



# Taco
- [得物从 0 到 1 自研客服 IM 系统的技术实践之路](http://www.52im.net/thread-4153-1-1.html)
- [参考链接](https://blog.csdn.net/jia12216/article/details/82699329)
## 背景
- 国内的推送割裂严重，稳定性和时效性难以保障，骑手测的触达率  91%+-；
- 商家侧轮询频率  5s，资源浪费严重；

## 冲突
- 接手项目辗转多手，没有文档，存在过度设计的问题；
- 业务侧催促得很急；

## 问题
- 关键性指标，触达率很低，触达时间较长；
- 整个服务链路不清晰，只有现象，缺少数据分析问题，大海捞针，无法定位问题；


## 解决方案
1. 梳理业务，从  SDK  到  Server ，梳理整条链路的关键节点；
2. 采集 SDK 关键节点的日志，并回收日志进行分析。如采集  SDK  建联操作、建联成功/失败、重试操作、重试次数、信息触达的通道、消息触达时间、消息处理时间等等；
3. 针对性解决  SDK  对应的问题；
4. 快速重试、智能心跳、结合厂商通道兜底等策略组合；

## 成果
### 公司项目
- 98%实现秒级触达,节省业务90%+的轮询服务。

### 个人
- 分析问题、制定策略、与业务协同排查问题的经验。


## 消息的可靠传递

### 不丢消息
- **ACK 机制：**
- 用户在发送消息的过程中都会携带一个msgid（32位的uuid，类似TCP的sequenceId），IM网关在接收到消息后，会根据msgid到数据库中查询是否存在该条消息，如果存在就不落库，如果不存在就落库。
- 然后再推送到接收方，接收方在收到消息后会回复ACK，ACK包中会携带上当前最新的seqid，IM网关收到ACK回复后会对最大的seqid进行更新。
-  这里为什么要更新最大seqid呢？这么设计肯定有一定道理的，IM网关在收到发送方发送的消息后除了到数据库中检测该消息是否存在外，还会对比当前接收到消息的seq和最大seqid两者之间的差值，会把 \[seq, seqid) 之间的数据全部推到接收方，正常情况下都是 \[n, n-1)，如果IM网关没有收到接收方ACK，n-1就不会更新，推送的消息个数就大于1了。如果seq和seqid相等那就是发送方重复推送的消息，这个时候就不会向接收方推送。这里就涉及到了消息重试，继续向下分析吧。
- 解决这个问题也是参考了TCP协议的重传机制。我们会在客服端、IM网关、用户端都维护一个超时计时器，一定时间内如果没有收到对方回的ACK包，会重新取出该消息进行重推。在重试一定次数后，如果还是没有收到ACK，视为放弃。


### 消息不重复
送方在发送消息时携带一个msgid，msgid是全局唯一的，针对同一条重推的消息msgid不变，接收方根据这个唯一的msgid进行去重，这样经过去重后，对于A来说，在聊天界面是不会看到重复的消息，不影响使用体验。


### 消息顺序不错乱




















# Answer
## 背景
- 【case】App  上线后，只有日志回捞一个途径获取 App  的运行日志，问题定位很难；
- 【case】发现问题很难，只能借助  server  的业务告警；
## 冲突
- 【困境】生产环境中设备没有实时的数据上报，缺乏监控，没有实时日志，无法快速发现问题、定位问题、解决问题；
- 【要求】稳定性要求高，需要  *1-5-10* 对应故障的 “1 分钟发现-5 分钟响应-10 分钟恢复”
## 问题
- 如何及时发现问题？
- 如何获取问题对应的日志？
	
## 解决方案
- 新起一套数据采集服务，包含采集规范、采集 SDK、数据收集服务、数据存储服务、数据展示服务；
- 将监控所需数据和日志所需数据进行融合，一式两用；
- 借助已有的  sls  实时日志能力，提供大规模，低成本的实时平台服务；
- 借助中台监控能力，统一客户端的监控，将稳定性纳入统一管理；
- 移动端提供采集  SDK，规范化、结构化、低成本地采集数据；

## 成果
### 公司内
- 项目推广于饿了么33个业务使用，且在各个业务发挥着至关重要的作用；
- 纳入到移动端标准的组件中，支撑着稳定性建设；

### 个人
1. 沟通能力，在前期的收集诉求，分析目标、制定方案中，锻炼了很多的沟通技巧和能力；
2. 在公司内的影响力得到了极大的提升，职级上得到提升；
3. 跨团队的协调能力，借助中台的力量、向业务推广的能力；


# 网络库的研发
## 背景
- 【现实】OKHttp  的成功率偏低；
- 【现实】3G  网络不稳定；
- 【现实】DNS 域名劫持严重；
- 【现实】存在大量的  H1  协议，直接升级，server 成本高；
- 【需求】配置需要快速下发到客户端；
- 【需求】物流有探索  quic  的需求；

## 冲突
1. 饿了么自有网关与集团隔离，短期内并不打算完全切换到弹内，网关无法统一
2. 网络库属于基础库，有很大的迁移成本、潜在很大的稳定性风险；
3. quic  的实现还未有最终的方案；
4. 域名未收敛；

## 问题
- 如何解决 H1  的头部阻塞？
- 如何解决业务对于 SDK 的感知，减少改造成本？
- 如何提高网络请求的成功率？
- 如何感知、解决域名被劫持？
- 基于何种技术栈实现网络协议？

## 解决方案
*短期*：
自研网络库，模拟实现  h2 以解决遇到的性能、稳定性问题；
*长期*：
- server  升级  H2
- 服务迁移弹内

**具体措施**
- 自研长连接通道、网络协议，客户端和网关维持长连接，网关解析协议，通过  RPC 转发到业务服务上；
- 基于开源方案进行自主开发迭代；
- 改造成本措施
	- hook  okhttp  请求发送的过程，在应用数据发送时，切换成自研 SDK；
	- 多通道兜底，get 2s 内长连接成功则取消短连接；
	- 状态码统一，细化请求链路节点，对于网关链路错误，可以再次重试，而无需关心幂等；
- 性能措施
	- 通过预热连接的方式降低网络建联的时长；
	- 通过在应用层添加竞速逻辑，以寻找高速连接，提高连接的通信性能；
	- 通过应用层  ping pong  策略，对连接进行保活，提高连接的复用率；
	- 通过和网关侧共同推进域名收敛，提高连接的复用率；
	- 通过监听设备的物理变化，如前后台切换、网络连通性监听，进行策略组合，改变连接的保活策略，如降低心跳  step，提高心跳的频率；
	- 多地部署网关，缩短网路的公网链路；
- 稳定性措施
	- 完善全局的监控；
	- DNS 监控；

## 成果
### 公司
- 长连接成功率  99%+;
- 短连接成功率 98%+;
- Android 比 OKHttp的成功率高 5 个百分点. iOS 比系统高一个多百分点.
- 连接耗时明显降低；
### 个人
- 深度参与了网络库的研发，对于网络编程有了更多的理解；
- 锻炼了，Cpp  和  JNI  相关的开发经验；

## Http2  和  Http3 
![[网络知识总结#H3|*]]
